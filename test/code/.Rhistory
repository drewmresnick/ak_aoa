paletteer,
pdftools,
plyr,
purrr,
raster,
RColorBrewer,
reshape2,
# rgdal,
rgeoda,
# rgeos,
rmapshaper,
rnaturalearth, # use devtools::install_github("ropenscilabs/rnaturalearth") if packages does not install properly
RSelenium,
sf,
shadowr,
sp,
stringr,
terra, # is replacing the raster package
tidyr,
tidyverse)
# Set directories
## Input directories
### study area geopackage
study_area_dir <- "C:/Users/Breanna.Xiong/Documents/R Scripts/ak_aoa/data/b_intermediate_data"
### raw data directory
raw_constraints_gpkg <- "C:/Users/Breanna.Xiong/Documents/R Scripts/ak_aoa/data/a_raw_data/constraints/constraints.gpkg"
code_dict <-list(
study_area = c("cordova",
"craig",
"juneau",
"ketchikan",
"kodiak",
"metlakatla",
"petersburg",
"seward",
"sitka",
"valdez",
"wrangell"),
code = c("co",
"cr",
"ju",
"ke",
"ko",
"me",
"pe",
"se",
"si",
"va",
"wa")
)
###################################
i <- 1
# loop through the data_codes list
dict_i <- i
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_i, "fiveacre_dissolve", sep = "_")
# loop through the data_codes list
dict_i <- code_dict$study_area[[i]]
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_i, "fiveacre_dissolve", sep = "_")
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_i, "/", dict_i, "_study_areas.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_i, "/", dict_i, "_study_areas.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_i, "/", dict_i, "_study_areas.gpkg", sep = ""))[[1]])])
# Set directories
## Input directories
### study area geopackage
study_area_dir <- "C:/Users/Breanna.Xiong/Documents/R Scripts/ak_aoa/data"
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_i, "fiveacre_dissolve", sep = "_")
print(paste(study_area_dir, "/", dict_i, "/", dict_i, "_study_areas.gpkg", sep = ""))
print(paste(study_area_dir, "/", dict_i, "/b_intermediate_data", dict_i, "_study_areas.gpkg", sep = ""))
print(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_areas.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_areas.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_areas.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_areas.gpkg", sep = ""))[[1]])])
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]])])
sf::st_read(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))
# Set directories
## Input directories
### study area geopackage
study_area_dir <- "C:/Users/Breanna.Xiong/Documents/R Scripts/ak_aoa/study_area"
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]])])
###################################
i <- 3
# loop through the data_codes list
dict_i <- code_dict$study_area[[i]]
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_i, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]])])
clean_data <- function(raw_data){
data_layer <- raw_data %>%
# reproject the same coordinate reference system (crs) as the study area
sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
# obtain data within hex study area
sf::st_intersection(hex_by_study)
return(data_layer)}
# Load Danger Zones and Restricted Areas and pass it through the cleaning function above
## source: https://marinecadastre.gov/downloads/data/mc/DangerZoneRestrictedArea.zip
## mapServer: https://coast.noaa.gov/arcgis/rest/services/OceanReports/DangerZonesAndRestrictedAreas/MapServer/0/metadata
### data
danger_zones <- sf::st_read(dsn = raw_constraints_gpkg, layer = "ak_cs_001") %>% clean_data()
print(dict_study)
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
print(dict_study)
dict_code <- code_dict$code[[i]]
print(dict_code)
code_dict <-list(
study_area = c("cordova",
"craig",
"juneau",
"ketchikan",
"kodiak",
"metlakatla",
"petersburg",
"seward",
"sitka",
"valdez",
"wrangell"),
code = c("ak_co",
"ak_cr",
"ak_ju",
"ak_ke",
"ak_ko",
"ak_me",
"ak_pe",
"ak_se",
"ak_si",
"ak_va",
"ak_wa")
)
dict_code <- code_dict$code[[i]]
print(dict_code)
for(i in 1:length(code_dict)){
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
print(dict_study)
dict_code <- code_dict$code[[i]]
print(dict_code)
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_i, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]])])
# clean_data <- function(raw_data){
#   data_layer <- raw_data %>%
#     # reproject the same coordinate reference system (crs) as the study area
#     sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
#     # obtain data within hex study area
#     sf::st_intersection(hex_by_study)
#   return(data_layer)}
# Load Danger Zones and Restricted Areas and pass it through the cleaning function above
## source: https://marinecadastre.gov/downloads/data/mc/DangerZoneRestrictedArea.zip
## mapServer: https://coast.noaa.gov/arcgis/rest/services/OceanReports/DangerZonesAndRestrictedAreas/MapServer/0/metadata
### data
danger_zones <- sf::st_read(dsn = raw_constraints_gpkg, layer = "ak_cs_001") %>% clean_data()
# Export data
## intermediate geopackage
# sf::st_write(obj = danger_zones, dsn = intermediate_gpkg, "ak_cs_001", append = F)
# # export data
# ## Get the file path that will write to your study area's (i.e. kodiak) b_intermediate_data folder.
# file <- paste(study_area_name, "_study_area.gpkg", sep = "")
# print(file)
#
# ## geopackage
# dissolved_gpkg <- file.path(study_area_dir, study_area_name, "b_intermediate_data/constraints", file)
# print(dissolved_gpkg)
#
# ## dissolved study area
# sf::st_write(obj = hex_dissolve, dsn = dissolved_gpkg, layer = paste(dataset, "dissolve", sep = "_"), append = F)
#
# ## undissolved study area
# sf::st_write(obj = hex_by_study, dsn = dissolved_gpkg, layer = paste(dataset), append = F)
}
for(i in 1:length(code_dict$study_area)){
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
print(dict_study)
dict_code <- code_dict$code[[i]]
print(dict_code)
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_i, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_i, "/b_intermediate_data/", dict_i, "_study_area.gpkg", sep = ""))[[1]])])
# clean_data <- function(raw_data){
#   data_layer <- raw_data %>%
#     # reproject the same coordinate reference system (crs) as the study area
#     sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
#     # obtain data within hex study area
#     sf::st_intersection(hex_by_study)
#   return(data_layer)}
# Load Danger Zones and Restricted Areas and pass it through the cleaning function above
## source: https://marinecadastre.gov/downloads/data/mc/DangerZoneRestrictedArea.zip
## mapServer: https://coast.noaa.gov/arcgis/rest/services/OceanReports/DangerZonesAndRestrictedAreas/MapServer/0/metadata
### data
# danger_zones <- sf::st_read(dsn = raw_constraints_gpkg, layer = "ak_cs_001") %>% clean_data()
# Export data
## intermediate geopackage
# sf::st_write(obj = danger_zones, dsn = intermediate_gpkg, "ak_cs_001", append = F)
# # export data
# ## Get the file path that will write to your study area's (i.e. kodiak) b_intermediate_data folder.
# file <- paste(study_area_name, "_study_area.gpkg", sep = "")
# print(file)
#
# ## geopackage
# dissolved_gpkg <- file.path(study_area_dir, study_area_name, "b_intermediate_data/constraints", file)
# print(dissolved_gpkg)
#
# ## dissolved study area
# sf::st_write(obj = hex_dissolve, dsn = dissolved_gpkg, layer = paste(dataset, "dissolve", sep = "_"), append = F)
#
# ## undissolved study area
# sf::st_write(obj = hex_by_study, dsn = dissolved_gpkg, layer = paste(dataset), append = F)
}
warnings()
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
print(dict_study)
dict_code <- code_dict$code[[i]]
print(dict_code)
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_study, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))
for(i in 1:length(code_dict$study_area)){
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
print(dict_study)
dict_code <- code_dict$code[[i]]
print(dict_code)
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_study, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]])])
# clean_data <- function(raw_data){
#   data_layer <- raw_data %>%
#     # reproject the same coordinate reference system (crs) as the study area
#     sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
#     # obtain data within hex study area
#     sf::st_intersection(hex_by_study)
#   return(data_layer)}
# Load Danger Zones and Restricted Areas and pass it through the cleaning function above
## source: https://marinecadastre.gov/downloads/data/mc/DangerZoneRestrictedArea.zip
## mapServer: https://coast.noaa.gov/arcgis/rest/services/OceanReports/DangerZonesAndRestrictedAreas/MapServer/0/metadata
### data
# danger_zones <- sf::st_read(dsn = raw_constraints_gpkg, layer = "ak_cs_001") %>% clean_data()
# Export data
## intermediate geopackage
# sf::st_write(obj = danger_zones, dsn = intermediate_gpkg, "ak_cs_001", append = F)
# # export data
# ## Get the file path that will write to your study area's (i.e. kodiak) b_intermediate_data folder.
# file <- paste(study_area_name, "_study_area.gpkg", sep = "")
# print(file)
#
# ## geopackage
# dissolved_gpkg <- file.path(study_area_dir, study_area_name, "b_intermediate_data/constraints", file)
# print(dissolved_gpkg)
#
# ## dissolved study area
# sf::st_write(obj = hex_dissolve, dsn = dissolved_gpkg, layer = paste(dataset, "dissolve", sep = "_"), append = F)
#
# ## undissolved study area
# sf::st_write(obj = hex_by_study, dsn = dissolved_gpkg, layer = paste(dataset), append = F)
}
for(i in 1:length(code_dict$study_area)){
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
print(dict_study)
dict_code <- code_dict$code[[i]]
print(dict_code)
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_study, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]])])
clean_data <- function(raw_data){
data_layer <- raw_data %>%
# reproject the same coordinate reference system (crs) as the study area
sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
# obtain data within hex study area
sf::st_intersection(hex_by_study)
return(data_layer)}
# Load Danger Zones and Restricted Areas and pass it through the cleaning function above
## source: https://marinecadastre.gov/downloads/data/mc/DangerZoneRestrictedArea.zip
## mapServer: https://coast.noaa.gov/arcgis/rest/services/OceanReports/DangerZonesAndRestrictedAreas/MapServer/0/metadata
### data
# danger_zones <- sf::st_read(dsn = raw_constraints_gpkg, layer = "ak_cs_001") %>% clean_data()
# Export data
## intermediate geopackage
# sf::st_write(obj = danger_zones, dsn = intermediate_gpkg, "ak_cs_001", append = F)
# # export data
# ## Get the file path that will write to your study area's (i.e. kodiak) b_intermediate_data folder.
# file <- paste(study_area_name, "_study_area.gpkg", sep = "")
# print(file)
#
# ## geopackage
# dissolved_gpkg <- file.path(study_area_dir, study_area_name, "b_intermediate_data/constraints", file)
# print(dissolved_gpkg)
#
# ## dissolved study area
# sf::st_write(obj = hex_dissolve, dsn = dissolved_gpkg, layer = paste(dataset, "dissolve", sep = "_"), append = F)
#
# ## undissolved study area
# sf::st_write(obj = hex_by_study, dsn = dissolved_gpkg, layer = paste(dataset), append = F)
}
clean_data <- function(raw_data){
data_layer <- raw_data %>%
# reproject the same coordinate reference system (crs) as the study area
sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
# obtain data within hex study area
sf::st_intersection(hex_by_study)
return(data_layer)}
for(i in 1:length(code_dict$study_area)){
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
print(dict_study)
dict_code <- code_dict$code[[i]]
print(dict_code)
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_study, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]])])
clean_data <- function(raw_data){
data_layer <- raw_data %>%
# reproject the same coordinate reference system (crs) as the study area
sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
# obtain data within hex study area
sf::st_intersection(hex_by_study)
return(data_layer)}
# Load Danger Zones and Restricted Areas and pass it through the cleaning function above
## source: https://marinecadastre.gov/downloads/data/mc/DangerZoneRestrictedArea.zip
## mapServer: https://coast.noaa.gov/arcgis/rest/services/OceanReports/DangerZonesAndRestrictedAreas/MapServer/0/metadata
### data
danger_zones <- sf::st_read(dsn = raw_constraints_gpkg, layer = "ak_cs_001") %>% clean_data()
# Export data
## intermediate geopackage
# sf::st_write(obj = danger_zones, dsn = intermediate_gpkg, "ak_cs_001", append = F)
# # export data
# ## Get the file path that will write to your study area's (i.e. kodiak) b_intermediate_data folder.
# file <- paste(study_area_name, "_study_area.gpkg", sep = "")
# print(file)
#
# ## geopackage
# dissolved_gpkg <- file.path(study_area_dir, study_area_name, "b_intermediate_data/constraints", file)
# print(dissolved_gpkg)
#
# ## dissolved study area
# sf::st_write(obj = hex_dissolve, dsn = dissolved_gpkg, layer = paste(dataset, "dissolve", sep = "_"), append = F)
#
# ## undissolved study area
# sf::st_write(obj = hex_by_study, dsn = dissolved_gpkg, layer = paste(dataset), append = F)
}
warnings()
paste(study_area_dir, study_area_name, "b_intermediate_data/constraints", file)
paste(study_area_dir, "/", dict_study, "/b_intermediate_data/constraints", file)
paste(study_area_dir, "/", dict_study, "/b_intermediate_data/constraints")
paste(study_area_dir, "/", dict_study, "/b_intermediate_data/constraints", sep = "")
paste(study_area_dir, "/", dict_study, "/b_intermediate_data/constraints", dict_code, sep = "")
paste(study_area_dir, "/", dict_study, "/b_intermediate_data/constraints/", dict_code, sep = "")
paste(study_area_dir, "/", dict_study, "/b_intermediate_data/constraints/constraints.gpkg", dict_code, sep = "")
paste(study_area_dir, dict_study, "b_intermediate_data/constraints/constraints.gpkg", dict_code, sep = "/")
# name split
parts <- strsplit("ak_cs_001", "_(?!.*_)", perl=TRUE)[[1]]
parts
parts[2]
# name split
id <- strsplit("ak_cs_001", "_(?!.*_)", perl=TRUE)[[1]]
paste(study_area_dir, dict_study, "b_intermediate_data/constraints/constraints.gpkg", dict_code, id[2], sep = "/")
paste(study_area_dir, dict_study, "b_intermediate_data/constraints/constraints.gpkg", paste(dict_code, id[2], sep="_"), sep = "/")
###################################
i <- 10
for(i in 1:length(code_dict$study_area)){
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
dict_code <- code_dict$code[[i]]
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_study, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]])])
clean_data <- function(raw_data){
data_layer <- raw_data %>%
# reproject the same coordinate reference system (crs) as the study area
sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
# obtain data within hex study area
sf::st_intersection(hex_by_study)
return(data_layer)}
# Load Danger Zones and Restricted Areas and pass it through the cleaning function above
## source: https://marinecadastre.gov/downloads/data/mc/DangerZoneRestrictedArea.zip
## mapServer: https://coast.noaa.gov/arcgis/rest/services/OceanReports/DangerZonesAndRestrictedAreas/MapServer/0/metadata
### data
danger_zones <- sf::st_read(dsn = raw_constraints_gpkg, layer = "ak_cs_001") %>% clean_data()
# name split
id <- strsplit("ak_cs_001", "_(?!.*_)", perl=TRUE)[[1]]
# Export data
## intermediate geopackage
# sf::st_write(obj = danger_zones, dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/constraints/constraints.gpkg", id[2], sep = ""), append = F)
paste(study_area_dir, dict_study, "b_intermediate_data/constraints/constraints.gpkg", paste(dict_code, id[2], sep="_"), sep = "/")
}
###################################
i <- 10
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
dict_code <- code_dict$code[[i]]
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_study, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]])])
clean_data <- function(raw_data){
data_layer <- raw_data %>%
# reproject the same coordinate reference system (crs) as the study area
sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
# obtain data within hex study area
sf::st_intersection(hex_by_study)
return(data_layer)}
# Load Danger Zones and Restricted Areas and pass it through the cleaning function above
## source: https://marinecadastre.gov/downloads/data/mc/DangerZoneRestrictedArea.zip
## mapServer: https://coast.noaa.gov/arcgis/rest/services/OceanReports/DangerZonesAndRestrictedAreas/MapServer/0/metadata
### data
danger_zones <- sf::st_read(dsn = raw_constraints_gpkg, layer = "ak_cs_001") %>% clean_data()
# name split
id <- strsplit("ak_cs_001", "_(?!.*_)", perl=TRUE)[[1]]
# Export data
## intermediate geopackage
sf::st_write(obj = danger_zones, dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/constraints/constraints.gpkg", id[2], sep = ""), append = F)
# Export data
## intermediate geopackage
sf::st_write(obj = danger_zones, dsn = paste(study_area_dir, dict_study, "b_intermediate_data/constraints/constraints.gpkg", paste(dict_code, id[2], sep="_"), sep = "/"), append = F)
# Export data
## intermediate geopackage
sf::st_write(obj = danger_zones, dsn = paste(study_area_dir, dict_study, "b_intermediate_data/constraints/constraints.gpkg", sep = "/"), paste(dict_code, id[2], sep="_"), append = F)
###################################
for(i in 1:length(code_dict$study_area)){
# loop through the data_codes list
dict_study <- code_dict$study_area[[i]]
dict_code <- code_dict$code[[i]]
# Get the name of the dataset in the geopackage
# Since there are 2 datasets that include the name of the study area, we have to read out the full name
dataset <- paste(dict_study, "fiveacre_dissolve", sep = "_")
sf::st_read(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the geopackage and look for the layers with specific names
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]])])
clean_data <- function(raw_data){
data_layer <- raw_data %>%
# reproject the same coordinate reference system (crs) as the study area
sf::st_transform("ESRI:102008") %>% # EPSG WKID 102008 (https://epsg.io/102008)
# obtain data within hex study area
sf::st_intersection(hex_by_study)
return(data_layer)}
# Load Danger Zones and Restricted Areas and pass it through the cleaning function above
## source: https://marinecadastre.gov/downloads/data/mc/DangerZoneRestrictedArea.zip
## mapServer: https://coast.noaa.gov/arcgis/rest/services/OceanReports/DangerZonesAndRestrictedAreas/MapServer/0/metadata
### data
danger_zones <- sf::st_read(dsn = raw_constraints_gpkg, layer = "ak_cs_001") %>% clean_data()
# name split
id <- strsplit("ak_cs_001", "_(?!.*_)", perl=TRUE)[[1]]
# Export data
## intermediate geopackage
sf::st_write(obj = danger_zones, dsn = paste(study_area_dir, dict_study, "b_intermediate_data/constraints/constraints.gpkg", sep = "/"), paste(dict_code, id[2], sep="_"), append = F)
}
### read in the geopackage
sf::st_read(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))
# for every element in the data_codes list, search through the hex grid geopackage and look for the layers with specific names "dataset" that we made above
hex_by_study <- sf::st_read(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""),
layer = sf::st_layers(paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]][grep(pattern = dataset,
sf::st_layers(dsn = paste(study_area_dir, "/", dict_study, "/b_intermediate_data/", dict_study, "_study_area.gpkg", sep = ""))[[1]])])
# name split
id <- strsplit("ak_cs_001", "_(?!.*_)", perl=TRUE)[[1]]
print(id)
